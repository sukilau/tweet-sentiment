{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function for text preprocessing\n",
    "def text_cleaning(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, \"lxml\").get_text() \n",
    "    text = re.sub(r'\\@\\w+',\"\", text)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "\n",
    "# Define a function to split a tweet into parsed sentences using NLTK's punkt tokenizer\n",
    "# note: Word2Vec take a list of sentences as input\n",
    "def tweet_to_sentences(tweet, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(tweet.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(text_cleaning(raw_sentence,remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Define functions to compute average feature vector\n",
    "def makeFeatureVec(tweet, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\n",
    "    for word in tweet:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(tweets, model, num_features):\n",
    "    counter = 0.\n",
    "    featureVecs = np.zeros((len(tweets),num_features),dtype=\"float32\")\n",
    "    for tweet in tweets:\n",
    "       featureVecs[counter] = makeFeatureVec(tweet, model,num_features)\n",
    "       counter = counter + 1.\n",
    "    return featureVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "This data set contains 2001 observations\n",
      "\n",
      "Parsing sentences ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://blip.fm/~5zposp7\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://hrimg.co.uk/view/pos15\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://tumblr.com/xmc21xqql\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://tinyurl.com/322t9k\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://tumblr.com/xdw1xgxhq\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://bit.ly/dATwK\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://tumblr.com/xgp1wwhmh\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://twitpic.com/6vposm6\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Word2Vec model ...\n",
      "\n",
      "\n",
      "Creating average feature vectors ...\n",
      "\n",
      "\n",
      "Any nan or infinite values of feature vectors? \n",
      "\n",
      "True\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:33: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "df = pd.read_csv(\"sentiment.tsv\", header=None, names=[\"sentiment\",\"tweet\"], delimiter=\"\\t\", quoting=2)\n",
    "print(\"This data set contains %d observations\" % df.shape[0])\n",
    "print()\n",
    "\n",
    "X = df[\"tweet\"]\n",
    "y = df[\"sentiment\"]\n",
    "y = preprocessing.LabelBinarizer().fit_transform(y)\n",
    "c, r = y.shape\n",
    "yvec = y.reshape(c,)\n",
    "\n",
    "\n",
    "# Parse tweets into sentences\n",
    "print(\"Parsing sentences ...\")\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = []\n",
    "for tweet in X:\n",
    "    sentences += tweet_to_sentences(tweet, tokenizer)\n",
    "# print(type(sentences), len(sentences), sentences[0:10])\n",
    "print()\n",
    "\n",
    "\n",
    "# Train Word2Vec\n",
    "num_features = 100                       \n",
    "min_word_count = 10                         \n",
    "num_workers = 4       \n",
    "context = 10                                                                                          \n",
    "downsampling = 1e-3 \n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "model = Word2Vec(sentences, workers=num_workers,size=num_features, min_count = min_word_count,\\\n",
    "                 window = context, sample = downsampling)\n",
    "model.init_sims(replace=True)\n",
    "model.save(\"300features_40minwords_10context\")\n",
    "# model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "# print(type(model.wv.syn0), len(model.wv.syn0), model.wv.syn0[0])\n",
    "# print(type(model.wv.index2word), len(model.wv.index2word))\n",
    "# print(model.wv.index2word)\n",
    "print()\n",
    "\n",
    "\n",
    "# Compute average feature vectors\n",
    "print(\"Creating average feature vectors ...\\n\")\n",
    "clean_X = []\n",
    "for tweet in X:\n",
    "    clean_X.append(text_cleaning(tweet, remove_stopwords=True))\n",
    "Xvec = getAvgFeatureVecs(clean_X, model, num_features)\n",
    "print()\n",
    "\n",
    "\n",
    "# Check if Xvec contains any nan or infinite value  \n",
    "print(\"Any nan or infinite values of feature vectors?\")\n",
    "print(np.any(np.isnan(Xvec))) #true\n",
    "print(np.all(np.isfinite(Xvec))) #false\n",
    "\n",
    "\n",
    "# Impute nan or infinite value\n",
    "Xvec[np.isnan(Xvec)] = np.median(Xvec[~np.isnan(Xvec)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest and finding the best parameter set ... \n",
      "Performing grid search...\n",
      "\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set :\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "Best score: 0.661\n",
      "Grid scores :\n",
      "0.597 (+/-0.095) for {'max_depth': 10, 'n_estimators': 10}\n",
      "0.661 (+/-0.122) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.584 (+/-0.070) for {'max_depth': None, 'n_estimators': 10}\n",
      "0.648 (+/-0.111) for {'max_depth': None, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search on Random Forest\n",
    "print(\"Training Random Forest and finding the best parameter set ... \")\n",
    "parameters_RF = {\"n_estimators\": [10, 100],\n",
    "#               \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"max_depth\": [10, None],\n",
    "#               \"min_samples_split\": sp_randint(1, 11),\n",
    "#               \"min_samples_leaf\": sp_randint(1, 11),\n",
    "#               \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "#               \"min_impurity_split\": [1e-07],\n",
    "#               \"bootstrap\": [True, False],            \n",
    "              }\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print()\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), parameters_RF, cv=10, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "grid_search.fit(Xvec, yvec)\n",
    "print(\"Best parameters set :\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Grid scores :\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
