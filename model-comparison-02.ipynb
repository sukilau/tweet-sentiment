{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "This data set contains 2001 observations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define function for text preprocessing\n",
    "def text_cleaning(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").get_text()  # remove html tag\n",
    "    text = re.sub(r'\\@\\w+',\"\", text)  # remove @tag \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # letters only\n",
    "    return text\n",
    "\n",
    "def text_preprocess(X):\n",
    "    clean_X = [] \n",
    "    for ind, val in X.iteritems():\n",
    "        clean_text = text_cleaning(val)\n",
    "        clean_X.append(clean_text)\n",
    "    return clean_X\n",
    "\n",
    "\n",
    "# load data\n",
    "print(\"Loading data ...\")\n",
    "df = pd.read_csv(\"sentiment.tsv\", header=None, names=[\"sentiment\",\"tweet\"], delimiter=\"\\t\", quoting=2)\n",
    "print(\"This data set contains %d observations\" % df.shape[0])\n",
    "print()\n",
    "\n",
    "X = df[\"tweet\"]\n",
    "y = df[\"sentiment\"]\n",
    "y = preprocessing.LabelBinarizer().fit_transform(y)\n",
    "c, r = y.shape\n",
    "yvec = y.reshape(c,)\n",
    "\n",
    "\n",
    "# text preprocessing\n",
    "X_clean = text_preprocess(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 3-dim feature vector using NLTK VADER Sentiment Intensity Analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_polarity(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    feature_vec =[]\n",
    "    feature_vec.append(score[\"neg\"])\n",
    "    feature_vec.append(score[\"neu\"])\n",
    "    feature_vec.append(score[\"pos\"])\n",
    "#     feature_vec.append(score[\"compound\"])\n",
    "    feature_vec = np.array(feature_vec)\n",
    "    return feature_vec\n",
    "\n",
    "Xvader = [vader_polarity(text) for text in X]\n",
    "\n",
    "\n",
    "# tokenize the tweets into count vectors using CountVectorizer \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None,stop_words = None, max_features = 5000) \n",
    "Xcountvec = vectorizer.fit_transform(X_clean)\n",
    "\n",
    "\n",
    "# normalize the count matrix to tf-idf representation using TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "Xtfidf  = tfidf.fit_transform(Xcountvec)\n",
    "\n",
    "\n",
    "# combining the two feature vectors \n",
    "Xvec = hstack((Xtfidf, Xvader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER + CountVectorizer + TfidfTransformer + LinearSVC\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Best parameters set :\n",
      "{'C': 0.1}\n",
      "Best score: 0.804\n",
      "Grid scores :\n",
      "0.804 (+/-0.059) for {'C': 0.1}\n",
      "0.798 (+/-0.064) for {'C': 0.5}\n",
      "0.790 (+/-0.066) for {'C': 1}\n",
      "0.775 (+/-0.069) for {'C': 3}\n",
      "0.768 (+/-0.070) for {'C': 5}\n",
      "\n",
      "VADER + CountVectorizer + TfidfTransformer + SGDClassifier  \n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set :\n",
      "{'alpha': 0.001}\n",
      "Best score: 0.806\n",
      "Grid scores :\n",
      "0.771 (+/-0.071) for {'alpha': 1e-05}\n",
      "0.776 (+/-0.068) for {'alpha': 0.0001}\n",
      "0.806 (+/-0.062) for {'alpha': 0.001}\n",
      "0.778 (+/-0.067) for {'alpha': 0.01}\n",
      "0.766 (+/-0.065) for {'alpha': 0.1}\n",
      "\n",
      "VADER + CountVectorizer + TfidfTransformer + RandomForestClassifier *\n",
      "Performing grid search...\n",
      "Parameters:\n",
      "{'n_estimators': [100, 500, 1000], 'max_depth': [10, None]}\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set :\n",
      "{'max_depth': None, 'n_estimators': 1000}\n",
      "Best score: 0.795\n",
      "Grid scores :\n",
      "0.781 (+/-0.053) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.791 (+/-0.065) for {'max_depth': 10, 'n_estimators': 500}\n",
      "0.791 (+/-0.061) for {'max_depth': 10, 'n_estimators': 1000}\n",
      "0.792 (+/-0.060) for {'max_depth': None, 'n_estimators': 100}\n",
      "0.795 (+/-0.060) for {'max_depth': None, 'n_estimators': 500}\n",
      "0.795 (+/-0.062) for {'max_depth': None, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(\"VADER + CountVectorizer + TfidfTransformer + LinearSVC\")\n",
    "parameters = {\"C\" : [0.1,0.5,1,3,5]}    \n",
    "grid_search = GridSearchCV(LinearSVC(), parameters, cv=10, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "grid_search.fit(Xvec, yvec)\n",
    "print(\"Best parameters set :\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Grid scores :\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"VADER + CountVectorizer + TfidfTransformer + SGDClassifier  \")\n",
    "parameters = {\"alpha\" : [1e-5,1e-4,1e-3,1e-2,0.1]}          \n",
    "grid_search = GridSearchCV(SGDClassifier(), parameters, cv=10, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "grid_search.fit(Xvec, yvec)\n",
    "print(\"Best parameters set :\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Grid scores :\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"VADER + CountVectorizer + TfidfTransformer + RandomForestClassifier *\")\n",
    "parameters = {\"n_estimators\": [100,500,1000],\n",
    "#               \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"max_depth\": [10, None],\n",
    "#               \"min_samples_split\": sp_randint(1, 11),\n",
    "#               \"min_samples_leaf\": sp_randint(1, 11),\n",
    "#               \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "#               \"min_impurity_split\": [1e-07],\n",
    "#               \"bootstrap\": [True, False],            \n",
    "              }\n",
    "print(\"Performing grid search...\")\n",
    "print(\"Parameters:\")\n",
    "print(parameters)\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), parameters, cv=10, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "grid_search.fit(Xvec, yvec)\n",
    "print(\"Best parameters set :\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Grid scores :\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
