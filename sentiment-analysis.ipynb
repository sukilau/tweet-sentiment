{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords \n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "# from sklearn.cross_validation import KFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function for text preprocessing\n",
    "def text_cleaning(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").get_text()  # remove html tag\n",
    "    text = re.sub(r'\\@\\w+',\"\", text)  # remove @tag \n",
    "#     text = re.sub(r'\\#\\w+',\"\", text)  \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # letters only\n",
    "#     words = [e.lower() for e in text.split() if len(e) >= 3]\n",
    "#     stops = set(stopwords.words(\"english\"))\n",
    "#     words = [w for w in words if not w in stops]  \n",
    "#     words = word_tokenize(text)\n",
    "#     stemmer = PorterStemmer()\n",
    "#     stemmer = LancasterStemmer()\n",
    "#     stemmer = SnowballStemmer(\"english\")\n",
    "#     words = [stemmer.stem(word) for word in text.split(\" \")]\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     words = [lemmatizer.lemmatize(word) for word in text.split(\" \")]\n",
    "#     text=\" \".join(words) \n",
    "    return text\n",
    "\n",
    "def text_preprocess(X):\n",
    "    clean_X = [] \n",
    "    for ind, val in X.iteritems():\n",
    "        clean_text = text_cleaning(val)\n",
    "        clean_X.append(clean_text)\n",
    "    return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "This data set contains 2001 observations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print(\"Loading data ...\")\n",
    "df = pd.read_csv(\"sentiment.tsv\", header=None, names=[\"sentiment\",\"tweet\"], delimiter=\"\\t\", quoting=2)\n",
    "print(\"This data set contains %d observations\" % df.shape[0])\n",
    "print()\n",
    "\n",
    "X = df[\"tweet\"]\n",
    "y = df[\"sentiment\"]\n",
    "y = preprocessing.LabelBinarizer().fit_transform(y)\n",
    "c, r = y.shape\n",
    "yvec = y.reshape(c,)\n",
    "\n",
    "\n",
    "# text preprocessing\n",
    "X_clean = text_preprocess(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create 3-dim feature vector using NLTK VADER Sentiment Intensity Analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_polarity(text):\n",
    "    score = vader.polarity_scores(text)\n",
    "    feature_vec =[]\n",
    "    feature_vec.append(score[\"neg\"])\n",
    "    feature_vec.append(score[\"neu\"])\n",
    "    feature_vec.append(score[\"pos\"])\n",
    "#     feature_vec.append(score[\"compound\"])\n",
    "    feature_vec = np.array(feature_vec)\n",
    "    return feature_vec\n",
    "\n",
    "Xvader = [vader_polarity(text) for text in X]\n",
    "\n",
    "\n",
    "# tokenize the tweets into count vectors using CountVectorizer \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None,stop_words = None, max_features = 5000) \n",
    "Xcountvec = vectorizer.fit_transform(X_clean)\n",
    "\n",
    "\n",
    "# normalize the count matrix to tf-idf representation using TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "Xtfidf  = tfidf.fit_transform(Xcountvec)\n",
    "\n",
    "\n",
    "# combining the two feature vectors \n",
    "Xvec = hstack((Xtfidf, Xvader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC and finding the best parameter set ... \n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters set :\n",
      "{'C': 0.5, 'kernel': 'linear'}\n",
      "\n",
      "Best score: 0.806\n",
      "Grid scores :\n",
      "\n",
      "0.768 (+/-0.065) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "0.765 (+/-0.061) for {'C': 0.1, 'kernel': 'poly'}\n",
      "0.791 (+/-0.070) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.768 (+/-0.065) for {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "0.768 (+/-0.066) for {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.765 (+/-0.061) for {'C': 0.5, 'kernel': 'poly'}\n",
      "0.806 (+/-0.062) for {'C': 0.5, 'kernel': 'linear'}\n",
      "0.768 (+/-0.066) for {'C': 0.5, 'kernel': 'sigmoid'}\n",
      "0.768 (+/-0.065) for {'C': 1, 'kernel': 'rbf'}\n",
      "0.765 (+/-0.061) for {'C': 1, 'kernel': 'poly'}\n",
      "0.800 (+/-0.070) for {'C': 1, 'kernel': 'linear'}\n",
      "0.768 (+/-0.066) for {'C': 1, 'kernel': 'sigmoid'}\n",
      "0.768 (+/-0.065) for {'C': 3, 'kernel': 'rbf'}\n",
      "0.765 (+/-0.061) for {'C': 3, 'kernel': 'poly'}\n",
      "0.776 (+/-0.068) for {'C': 3, 'kernel': 'linear'}\n",
      "0.768 (+/-0.065) for {'C': 3, 'kernel': 'sigmoid'}\n",
      "0.768 (+/-0.065) for {'C': 5, 'kernel': 'rbf'}\n",
      "0.765 (+/-0.061) for {'C': 5, 'kernel': 'poly'}\n",
      "0.765 (+/-0.066) for {'C': 5, 'kernel': 'linear'}\n",
      "0.768 (+/-0.065) for {'C': 5, 'kernel': 'sigmoid'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grid search on SVC to find the best parameter set and evaluate using 10-fold cross validation\n",
    "print(\"Training SVC and finding the best parameter set ... \")\n",
    "parameters = {\"C\" : [0.1,0.5,1,3,5], \"kernel\":[\"rbf\",\"poly\",\"linear\", \"sigmoid\"]}   \n",
    "grid_search = GridSearchCV(SVC(), parameters, cv=10, scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n",
    "grid_search.fit(Xvec, yvec)\n",
    "print()\n",
    "print(\"Best parameters set :\")\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Grid scores :\")\n",
    "print()\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 4864)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
